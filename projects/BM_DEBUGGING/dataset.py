"""
BM_DEBUGGING Dataset Implementation

This dataset is specifically designed for the BM_DEBUGGING project.
It handles synthetic data generated by the data generator, which creates
timestamped datasets with ground truth model parameters for debugging.

Key Features:
- Automatically finds most recent generated dataset
- Loads ground truth model parameters for validation
- Provides debugging utilities
- Validates data format (spin values: {-1, +1})
"""

import pandas as pd
import numpy as np
import torch
from pathlib import Path
from typing import Optional, Tuple, Dict, Any
from bm_core.models.dataset import BMDataset


class DebugDataset(BMDataset):
    """
    Dataset for BM_DEBUGGING project with ground truth parameters.

    This dataset extends BMDataset to provide debugging-specific functionality:
    - Loads from timestamped data directories
    - Reads ground truth model parameters
    - Validates synthetic data format
    - Provides comparison utilities

    The data generator creates directories like:
        projects/BM_DEBUGGING/data/test_run_20251205_183915/
        ├── test_run.csv              # Synthetic samples
        ├── true_model_params.pt      # Ground truth parameters
        └── metadata.txt              # Generation metadata

    Usage:
        # Option 1: Auto-load most recent dataset
        dataset = DebugDataset("auto")

        # Option 2: Load specific timestamped directory
        dataset = DebugDataset("projects/BM_DEBUGGING/data/test_run_20251205_183915/test_run.csv")

        # Option 3: Load specific CSV directly
        dataset = DebugDataset("path/to/specific/data.csv")

        # Access ground truth parameters
        true_params = dataset.get_true_parameters()
        print(f"True linear: {true_params['linear']}")
        print(f"True quadratic: {true_params['quadratic']}")
    """

    def __init__(self, csv_path: str, **kwargs):
        """
        Initialize debugging dataset.

        Args:
            csv_path: Path to CSV file, or "auto" to auto-load most recent
            **kwargs: Additional arguments passed to parent BMDataset
        """
        # Handle "auto" mode - find most recent dataset
        if csv_path == "auto":
            csv_path = self._find_latest_dataset()
            print(f"\n[Auto-mode] Using latest dataset: {csv_path}")

        # Store paths before parent init
        self.csv_path = csv_path
        self.data_dir = Path(csv_path).parent
        self.true_params_path = self.data_dir / "true_model_params.pt"
        self.metadata_path = self.data_dir / "metadata.txt"

        # Load ground truth parameters if available
        self.true_params = None
        self.metadata = None
        if self.true_params_path.exists():
            self._load_true_parameters()
            self._load_metadata()

        # Call parent init (this calls our load_data method)
        super().__init__(csv_path, **kwargs)

        # Validate data after loading
        self._validate_synthetic_data()

    def _find_latest_dataset(self) -> str:
        """
        Find the most recent generated dataset in BM_DEBUGGING/data/.

        Returns:
            Path to the CSV file in the most recent timestamped directory

        Raises:
            FileNotFoundError: If no datasets found
        """
        data_dir = Path(__file__).parent / "data"

        if not data_dir.exists():
            raise FileNotFoundError(
                f"Data directory not found: {data_dir}\n"
                f"Generate data first using: python projects/BM_DEBUGGING/utils/run_generator.py"
            )

        # Find all timestamped directories (format: *_YYYYMMDD_HHMMSS)
        timestamped_dirs = [
            d for d in data_dir.iterdir()
            if d.is_dir() and '_' in d.name
        ]

        if not timestamped_dirs:
            raise FileNotFoundError(
                f"No datasets found in {data_dir}\n"
                f"Generate data first using: python projects/BM_DEBUGGING/utils/run_generator.py"
            )

        # Sort by directory name (timestamp is in the name)
        latest_dir = sorted(timestamped_dirs, key=lambda d: d.name)[-1]

        # Find CSV file in directory
        csv_files = list(latest_dir.glob("*.csv"))

        if not csv_files:
            raise FileNotFoundError(
                f"No CSV file found in {latest_dir}"
            )

        return str(csv_files[0])

    def _load_true_parameters(self):
        """Load ground truth model parameters from disk."""
        try:
            self.true_params = torch.load(
                self.true_params_path,
                map_location='cpu',
                weights_only=False
            )
            print(f"\n[Debug] Loaded true model parameters:")
            print(f"  Linear bias shape: {self.true_params['linear'].shape}")
            print(f"  Quadratic weight shape: {self.true_params['quadratic'].shape}")
            print(f"  Nodes: {len(self.true_params['nodes'])}")
            print(f"  Edges: {len(self.true_params['edges'])}")

        except Exception as e:
            print(f"\n[Warning] Could not load true parameters: {e}")
            self.true_params = None

    def _load_metadata(self):
        """Load generation metadata from disk."""
        try:
            with open(self.metadata_path, 'r') as f:
                self.metadata = f.read()
            print(f"\n[Debug] Dataset metadata:")
            print(self.metadata)

        except Exception as e:
            print(f"\n[Warning] Could not load metadata: {e}")
            self.metadata = None

    def load_data(self, csv_path: str) -> np.ndarray:
        """
        Load synthetic data from CSV.

        The data generator creates CSV files with:
        - Columns v0, v1, ..., vN for visible units
        - Column sample_id for tracking
        - Values in spin format: {-1, +1}

        Args:
            csv_path: Path to CSV file

        Returns:
            Numpy array of shape (n_samples, n_visible) with dtype float32
        """
        # Load CSV
        df = pd.read_csv(csv_path)

        print(f"\n[Debug] Loading synthetic dataset:")
        print(f"  Path: {csv_path}")
        print(f"  Total samples: {len(df)}")
        print(f"  Columns: {df.columns.tolist()}")

        # Extract visible unit columns (v0, v1, v2, ...)
        visible_cols = [col for col in df.columns if col.startswith('v')]

        if not visible_cols:
            raise ValueError(
                f"No visible columns (v0, v1, ...) found in {csv_path}. "
                f"Available columns: {df.columns.tolist()}"
            )

        # Extract data
        data = df[visible_cols].values.astype(np.float32)

        print(f"  Visible units: {len(visible_cols)}")
        print(f"  Data shape: {data.shape}")
        print(f"  Value range: [{data.min():.1f}, {data.max():.1f}]")

        # Sample values
        print(f"\n[Debug] First 3 samples:")
        for i in range(min(3, len(data))):
            print(f"  Sample {i}: {data[i][:10]}...")  # Show first 10 values

        return data

    def _validate_synthetic_data(self):
        """
        Validate that data is in expected spin format.

        Synthetic data should have values in {-1, +1}.
        """
        unique_values = np.unique(self.data)

        # Check if values are approximately -1 and +1
        expected_values = {-1.0, 1.0}
        actual_values = set(unique_values)

        if not actual_values.issubset({-1.0, 1.0}):
            print(f"\n[Warning] Data values outside spin format {{-1, +1}}: {actual_values}")
            print(f"  This may indicate non-spin data or preprocessing issues.")
        else:
            print(f"\n[Debug] Data validation passed: Values in spin format {{-1, +1}}")

    def get_true_parameters(self) -> Optional[Dict[str, Any]]:
        """
        Get ground truth model parameters.

        Returns:
            Dictionary with keys:
                - 'linear': Linear bias tensor
                - 'quadratic': Quadratic weight tensor
                - 'nodes': Node indices
                - 'edges': Edge list
                - 'hidden_nodes': Hidden node indices (if applicable)
                - 'config': True model configuration
            Or None if parameters not available
        """
        return self.true_params

    def get_metadata(self) -> Optional[str]:
        """
        Get dataset generation metadata.

        Returns:
            Metadata string or None if not available
        """
        return self.metadata

    def compare_parameters(
        self,
        learned_linear: torch.Tensor,
        learned_quadratic: torch.Tensor,
        verbose: bool = True
    ) -> Dict[str, float]:
        """
        Compare learned parameters with ground truth.

        Useful for debugging: check how close the trained model
        is to the true model that generated the data.

        Args:
            learned_linear: Learned linear biases
            learned_quadratic: Learned quadratic weights
            verbose: Print comparison results

        Returns:
            Dictionary with error metrics:
                - 'linear_mse': MSE between linear biases
                - 'linear_mae': MAE between linear biases
                - 'quadratic_mse': MSE between quadratic weights
                - 'quadratic_mae': MAE between quadratic weights
                - 'linear_corr': Correlation between linear biases
                - 'quadratic_corr': Correlation between quadratic weights

        Raises:
            ValueError: If true parameters not available
        """
        if self.true_params is None:
            raise ValueError(
                "True parameters not available. "
                "Make sure dataset was generated with true_model_params.pt"
            )

        true_linear = self.true_params['linear']
        true_quadratic = self.true_params['quadratic']

        # Ensure same device
        device = learned_linear.device
        true_linear = true_linear.to(device)
        true_quadratic = true_quadratic.to(device)

        # Calculate metrics
        linear_mse = torch.mean((learned_linear - true_linear) ** 2).item()
        linear_mae = torch.mean(torch.abs(learned_linear - true_linear)).item()

        quadratic_mse = torch.mean((learned_quadratic - true_quadratic) ** 2).item()
        quadratic_mae = torch.mean(torch.abs(learned_quadratic - true_quadratic)).item()

        # Correlation
        linear_corr = torch.corrcoef(
            torch.stack([learned_linear.flatten(), true_linear.flatten()])
        )[0, 1].item()

        quadratic_corr = torch.corrcoef(
            torch.stack([learned_quadratic.flatten(), true_quadratic.flatten()])
        )[0, 1].item()

        metrics = {
            'linear_mse': linear_mse,
            'linear_mae': linear_mae,
            'quadratic_mse': quadratic_mse,
            'quadratic_mae': quadratic_mae,
            'linear_corr': linear_corr,
            'quadratic_corr': quadratic_corr
        }

        if verbose:
            print(f"\n{'='*70}")
            print("PARAMETER COMPARISON: Learned vs True")
            print(f"{'='*70}")
            print(f"\nLinear Biases:")
            print(f"  MSE:         {linear_mse:.6f}")
            print(f"  MAE:         {linear_mae:.6f}")
            print(f"  Correlation: {linear_corr:.6f}")
            print(f"\nQuadratic Weights:")
            print(f"  MSE:         {quadratic_mse:.6f}")
            print(f"  MAE:         {quadratic_mae:.6f}")
            print(f"  Correlation: {quadratic_corr:.6f}")
            print(f"{'='*70}")

        return metrics


# =============================================================================
# Helper Functions for BM_DEBUGGING
# =============================================================================

def load_latest_dataset(auto: bool = True) -> DebugDataset:
    """
    Convenience function to load the most recent dataset.

    Args:
        auto: If True, automatically find latest dataset

    Returns:
        DebugDataset instance

    Example:
        dataset = load_latest_dataset()
        true_params = dataset.get_true_parameters()
    """
    return DebugDataset("auto" if auto else None)


def list_available_datasets() -> list[Tuple[str, str]]:
    """
    List all available datasets in BM_DEBUGGING/data/.

    Returns:
        List of tuples (directory_name, csv_path)
    """
    data_dir = Path(__file__).parent / "data"

    if not data_dir.exists():
        return []

    datasets = []
    for d in sorted(data_dir.iterdir()):
        if d.is_dir():
            csv_files = list(d.glob("*.csv"))
            if csv_files:
                datasets.append((d.name, str(csv_files[0])))

    return datasets


def print_dataset_summary():
    """
    Print summary of all available datasets.

    Example:
        from projects.BM_DEBUGGING.dataset import print_dataset_summary
        print_dataset_summary()
    """
    datasets = list_available_datasets()

    if not datasets:
        print("\nNo datasets found in projects/BM_DEBUGGING/data/")
        print("Generate data using: python projects/BM_DEBUGGING/utils/run_generator.py")
        return

    print(f"\n{'='*70}")
    print(f"Available Datasets in BM_DEBUGGING ({len(datasets)} found)")
    print(f"{'='*70}")

    for i, (dir_name, csv_path) in enumerate(datasets, 1):
        metadata_path = Path(csv_path).parent / "metadata.txt"

        print(f"\n{i}. {dir_name}")
        print(f"   CSV: {csv_path}")

        if metadata_path.exists():
            # Read first few lines of metadata
            with open(metadata_path, 'r') as f:
                lines = f.readlines()
                for line in lines[3:8]:  # Skip header, show key info
                    print(f"   {line.strip()}")

    print(f"\n{'='*70}")
    print(f"To use latest dataset:")
    print(f'  dataset = DebugDataset("auto")')
    print(f"{'='*70}\n")
