# Boltzmann Machine Pipeline Configuration

# Random seed for reproducibility
seed: 42

# Device configuration
device:
  use_cuda: "auto"  # "auto", "cuda", "cpu", or specific device like "cuda:0"
  # auto: automatically detect and use GPU if available, otherwise CPU

# True Model Configuration (data generation)
true_model:
  n_visible: 4
  n_hidden: 0
  model_type: "fvbm"  # "fvbm" (Fully Visible BM) | "sbm" (Standard BM) | "rbm" (Restricted BM)
  connectivity: "dense"  # "dense" | "sparse"
  connectivity_density: 0.7  # Only used if connectivity is "sparse" (0.0-1.0)

  # Parameter initialization
  linear_bias_scale: 1.0
  quadratic_weight_scale: 1.5

# Data Generation
data:
  dataset_name: "bm_toy_dataset"
  n_samples: 10000
  num_reads: 5000  # Samples per D-Wave API call
  prefactor: 1.0

  # Train/Val/Test split
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

  # Sampler configuration for data generation
  sampler:
    type: "simulated_annealing"
    # Classical: "simulated_annealing", "tabu", "steepest_descent", "greedy", "exact", "random"
    # Quantum: "dwave", "advantage"  (requires D-Wave Leap account)
    # Hybrid: "hybrid", "kerberos"  (requires D-Wave Leap account)
    params:
      beta_range: [1.0, 1.0]
      proposal_acceptance_criteria: "Gibbs"
      num_reads: 5000
      # For D-Wave samplers:
      # solver: null  # Optional: specify solver name
      # time_limit: 5  # For hybrid samplers (seconds)

# Learned Model Configuration
learned_model:
  # Should match true model topology
  n_visible: 4
  n_hidden: 0
  model_type: "fvbm"  # "fvbm" (Fully Visible BM) or "rbm" (Restricted BM)
  connectivity: "dense"  # "dense" or "sparse"
  connectivity_density: 0.7  # Only used if connectivity is "sparse" (0.0-1.0)

  # Initial parameter scale (smaller than true model)
  init_linear_scale: 0.1
  init_quadratic_scale: 0.1

# Training Configuration
training:
  batch_size: 10000
  n_epochs: 500
  learning_rate: 0.01
  optimizer: "adam"  # "sgd" or "adam"

  # Model sampling during training
  model_sample_size: 1000
  prefactor: 1.0

  # Sampler configuration for training
  sampler:
    type: "simulated_annealing"
    # Classical: "simulated_annealing", "tabu", "steepest_descent", "greedy", "exact", "random"
    # Quantum: "dwave", "advantage"  (requires D-Wave Leap account)
    # Hybrid: "hybrid", "kerberos"  (requires D-Wave Leap account)
    params:
      beta_range: [1.0, 1.0]
      proposal_acceptance_criteria: "Gibbs"
      # For D-Wave samplers:
      # solver: null  # Optional: specify solver name
      # time_limit: 5  # For hybrid samplers (seconds)

  # Optimizer parameters
  optimizer_params:
    betas: [0.85, 0.999]  # Slightly lower momentum for noisy BM gradients
    eps: 1.0e-7
    weight_decay: 0.0     # Use dedicated regularization instead

  # Gradient clipping (prevents divergence)
  gradient_clipping:
    enabled: true
    method: "norm"        # "norm" or "value"
    max_norm: 1.0         # Maximum gradient norm
    max_value: 0.5        # Maximum gradient value (if method="value")

  # Regularization (prevents overfitting and unbounded parameters)
  regularization:
    linear_l2: 0.001      # L2 penalty on linear biases
    quadratic_l2: 0.01    # L2 penalty on quadratic weights
    quadratic_l1: 0.00    # L1 penalty for sparsity (optional)

  # Learning rate scheduling
  lr_scheduler:
    enabled: true
    type: "plateau"       # "plateau", "step", "cosine", or "exponential"
    # Plateau-specific params
    factor: 0.5           # Reduce LR by this factor
    patience: 15          # Epochs without improvement before reducing
    min_lr: 1.0e-5        # Minimum learning rate
    monitor: "val_loss"   # Metric to monitor
    # Step-specific params
    step_size: 100        # For type="step"
    gamma: 0.5            # For type="step" or "exponential"
    # Cosine-specific params
    T_max: 50             # For type="cosine"
    eta_min: 1.0e-5       # For type="cosine"

  # Early stopping
  early_stopping:
    enabled: false              # Enable/disable early stopping
    patience: 20                # Number of epochs without improvement before stopping
    min_delta: 0.0001           # Minimum change to qualify as improvement
    metric: "val_loss"          # Metric to monitor: "val_loss", "train_loss", "grad_norm"
    mode: "min"                 # "min" for loss, "max" for accuracy-like metrics
    restore_best_weights: true  # Restore best model weights after early stopping

  # Hidden unit handling (only if n_hidden > 0)
  hidden_kind: null  # "exact-disc", "sampling", or null (null for fully visible BM)

  # Checkpointing
  save_best_model: true
  checkpoint_dir: "outputs/checkpoints"

# Logging and Visualization
logging:
  log_interval: 1  # Log every N epochs
  save_plots: true
  plot_dir: "outputs/plots"

  # Metrics to track
  track_metrics:
    - "loss"
    - "grad_norm"
    - "beta"
    - "val_loss"

# Paths (relative to bm_pipeline/ directory when running main.py directly,
#        or relative to project root when using run scripts)
paths:
  data_dir: "outputs/data"
  model_dir: "outputs/models"
  log_dir: "outputs/logs"
