# Boltzmann Machine Pipeline Configuration

# Random seed for reproducibility
seed: 42

# Device configuration
device:
  use_cuda: "auto"  # "auto", "cuda", "cpu", or specific device like "cuda:0"
  # auto: automatically detect and use GPU if available, otherwise CPU

# True Model Configuration (data generation)
true_model:
  n_visible: 4
  n_hidden: 0
  model_type: "fvbm"  # "fvbm" (Fully Visible BM) | "sbm" (Standard BM) | "rbm" (Restricted BM)
  connectivity: "dense"  # "dense" | "sparse"
  connectivity_density: 0.7  # Only used if connectivity is "sparse" (0.0-1.0)

  # Parameter initialization
  linear_bias_scale: 1.0
  quadratic_weight_scale: 1.5

# Data Generation
data:
  dataset_name: "bm_toy_dataset"
  n_samples: 5000
  prefactor: 1.0

  # Train/Val/Test split
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

  # Sampler configuration for data generation
  sampler:
    type: "simulated_annealing"
    # ===== AVAILABLE SAMPLERS =====
    # Classical (Free, Local):
    #   "gibbs"                - Gibbs MCMC sampler (best for probability sampling)
    #   "simulated_annealing"  - MCMC-based annealing (good general purpose)
    #   "steepest_descent"     - Local search (fast, local optima)
    #   "exact"                - Brute force (only for N â‰¤ 20 variables)
    #   "random"               - Uniform random (baseline)
    #   "tabu"                 - Tabu search (has parameter issues)
    #   "greedy"               - Greedy heuristic (not available in current D-Wave version)
    #
    # Quantum (Requires D-Wave Leap Account):
    #   "dwave"                - D-Wave quantum annealer (QPU)
    #   "advantage"            - D-Wave Advantage system (alias for dwave)
    #
    # Hybrid (Requires D-Wave Leap Account):
    #   "hybrid"               - LeapHybridSampler (general purpose)
    #   "hybrid_bqm"           - LeapHybridBQMSampler (alias for hybrid)
    #   "hybrid_dqm"           - LeapHybridDQMSampler (discrete problems)
    #   "kerberos"             - KerberosSampler (QPU + classical hybrid)
    params:
      # Common parameters:
      num_reads: 5000  # Number of samples to generate

      # Gibbs sampler parameters:
      # num_sweeps: 1000         # Number of MCMC sweeps (default: 1000)
      # burn_in: 100             # Burn-in period (default: 100)
      # thinning: 1              # Keep every nth sample (default: 1)
      # randomize_order: true    # Randomize variable update order (default: true)

      # Simulated Annealing parameters:
      beta_range: [1.0, 1.0]                    # Temperature schedule [beta_min, beta_max]
      proposal_acceptance_criteria: "Gibbs"     # "Gibbs" or "Metropolis"

      # Tabu sampler parameters:
      # tenure: null             # Tabu list size (default: auto)
      # timeout: 20              # Timeout in seconds (default: 20)

      # D-Wave quantum/hybrid parameters:
      # solver: null             # Optional: specify solver name
      # time_limit: 5            # For hybrid samplers (seconds)

# Learned Model Configuration
learned_model:
  # Should match true model topology
  n_visible: 4
  n_hidden: 0
  model_type: "fvbm"  # "fvbm" (Fully Visible BM) or "rbm" (Restricted BM)
  connectivity: "dense"  # "dense" or "sparse"
  connectivity_density: 0.7  # Only used if connectivity is "sparse" (0.0-1.0)

  # Initial parameter scale (smaller than true model)
  init_linear_scale: 0.1
  init_quadratic_scale: 0.1

# Training Configuration
training:
  batch_size: 10000
  n_epochs: 500
  learning_rate: 0.01
  optimizer: "adam"  # "sgd" or "adam"

  # Model sampling during training
  model_sample_size: 1000
  prefactor: 1.0

  # Sampler configuration for training
  sampler:
    type: "simulated_annealing"
    # ===== AVAILABLE SAMPLERS =====
    # See data generation section above for full list and descriptions
    # Recommended for training: "gibbs", "simulated_annealing", or "exact" (small models)
    params:
      # Simulated Annealing (default):
      beta_range: [1.0, 1.0]
      proposal_acceptance_criteria: "Gibbs"

      # To use Gibbs instead, uncomment:
      # num_sweeps: 1000
      # burn_in: 100

  # Optimizer parameters
  optimizer_params:
    betas: [0.85, 0.999]  # Slightly lower momentum for noisy BM gradients
    eps: 1.0e-7
    weight_decay: 0.0     # Use dedicated regularization instead

  # Gradient clipping (prevents divergence)
  gradient_clipping:
    enabled: true
    method: "norm"        # "norm" or "value"
    max_norm: 1.0         # Maximum gradient norm
    max_value: 0.5        # Maximum gradient value (if method="value")

  # Regularization (prevents overfitting and unbounded parameters)
  regularization:
    linear_l2: 0.001      # L2 penalty on linear biases
    quadratic_l2: 0.01    # L2 penalty on quadratic weights
    quadratic_l1: 0.00    # L1 penalty for sparsity (optional)

  # Learning rate scheduling
  lr_scheduler:
    enabled: true
    type: "plateau"       # "plateau", "step", "cosine", or "exponential"
    # Plateau-specific params
    factor: 0.5           # Reduce LR by this factor
    patience: 15          # Epochs without improvement before reducing
    min_lr: 1.0e-5        # Minimum learning rate
    monitor: "val_loss"   # Metric to monitor
    # Step-specific params
    step_size: 100        # For type="step"
    gamma: 0.5            # For type="step" or "exponential"
    # Cosine-specific params
    T_max: 50             # For type="cosine"
    eta_min: 1.0e-5       # For type="cosine"

  # Early stopping
  early_stopping:
    enabled: false              # Enable/disable early stopping
    patience: 20                # Number of epochs without improvement before stopping
    min_delta: 0.0001           # Minimum change to qualify as improvement
    metric: "val_loss"          # Metric to monitor: "val_loss", "train_loss", "grad_norm"
    mode: "min"                 # "min" for loss, "max" for accuracy-like metrics
    restore_best_weights: true  # Restore best model weights after early stopping

  # Hidden unit handling (only if n_hidden > 0)
  hidden_kind: null  # "exact-disc", "sampling", or null (null for fully visible BM)

  # Checkpointing
  save_best_model: true
  checkpoint_dir: "outputs/checkpoints"

# Logging and Visualization
logging:
  log_interval: 1  # Log every N epochs
  save_plots: true
  plot_dir: "outputs/plots"

  # Metrics to track
  track_metrics:
    - "loss"
    - "grad_norm"
    - "beta"
    - "val_loss"

# Benchmark Configuration
# Benchmarks samplers by computing KL divergence from true Boltzmann distribution
benchmark:
  # Samplers to benchmark
  # See data generation section above for full list of available samplers
  # Recommended: "gibbs", "simulated_annealing", "steepest_descent", "exact", "random"
  samplers:
    - "gibbs"                 # Gibbs MCMC (theoretically correct, best quality)
    - "simulated_annealing"   # SA baseline (good for optimization)
    - "steepest_descent"      # Fast local search
    - "exact"                 # Brute force (slow for N > 10)
    - "random"                # Uniform random baseline

  # Range of problem sizes (number of variables)
  n_variables_range: [2, 3, 4, 5, 6, 7, 8, 9, 10]

  # Number of samples per benchmark run
  n_samples: 10000

  # Model configuration for benchmarking (always dense FVBM)
  model_type: "fvbm"
  connectivity: "dense"
  linear_bias_scale: 1.0
  quadratic_weight_scale: 1.5

# Paths (relative to bm_pipeline/ directory when running main.py directly,
#        or relative to project root when using run scripts)
paths:
  data_dir: "outputs/data"
  model_dir: "outputs/models"
  log_dir: "outputs/logs"
