# Boltzmann Machine Pipeline Configuration

# Random seed for reproducibility
seed: 42

# Device configuration
device:
  use_cuda: "auto"  # "auto", "cuda", "cpu", or specific device like "cuda:0"
  # auto: automatically detect and use GPU if available, otherwise CPU

# True Model Configuration (data generation)
true_model:
  n_visible: 10
  n_hidden: 0
  model_type: "fvbm"  # "fvbm" (Fully Visible BM) | "sbm" (Standard BM) | "rbm" (Restricted BM)
  connectivity: "dense"  # "dense" | "sparse"
  connectivity_density: 0.7  # Only used if connectivity is "sparse" (0.0-1.0)

  # Parameter initialization
  linear_bias_scale: 1.0
  quadratic_weight_scale: 1.5

# Data Generation
data:
  dataset_name: "bm_toy_dataset"
  n_samples: 5000
  prefactor: 1.0

  # Train/Val/Test split
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

  # Sampler configuration for data generation
  sampler:
    type: "gibbs"
    # ===== AVAILABLE SAMPLERS =====
    # Classical MCMC (Free, Local):
    #   "gibbs"                - Gibbs MCMC sampler (best for probability sampling)
    #   "metropolis"           - Metropolis-Hastings MCMC (temperature-controlled)
    #   "parallel_tempering"   - Parallel Tempering (replica exchange, best for complex landscapes)
    #   "simulated_annealing"  - MCMC-based annealing (good general purpose)
    #
    # Exact/Quasi-Exact (Free, Local, small N only):
    #   "exact"                - Brute force exact sampling (only for N ≤ 20 variables)
    #   "gumbel_max"           - Gumbel-max exact sampling (independent samples, N ≤ 20)
    #
    # Local Search/Optimization (Free, Local):
    #   "steepest_descent"     - Local search (fast, local optima)
    #   "tabu"                 - Tabu search (has parameter issues)
    #   "greedy"               - Greedy heuristic
    #
    # Baseline:
    #   "random"               - Uniform random (baseline)
    #   "greedy"               - Greedy heuristic (not available in current D-Wave version)
    #
    # Quantum (Requires D-Wave Leap Account):
    #   "dwave"                - D-Wave quantum annealer (QPU)
    #   "advantage"            - D-Wave Advantage system (alias for dwave)
    #
    # Hybrid (Requires D-Wave Leap Account):
    #   "hybrid"               - LeapHybridSampler (general purpose)
    #   "hybrid_bqm"           - LeapHybridBQMSampler (alias for hybrid)
    #   "hybrid_dqm"           - LeapHybridDQMSampler (discrete problems)
    #   "kerberos"             - KerberosSampler (QPU + classical hybrid)
    params:
      # Common parameters:
      num_reads: 5000  # Number of samples to generate

      # Gibbs sampler parameters:
      num_sweeps: 1000         # Number of MCMC sweeps (default: 1000)
      burn_in: 100             # Burn-in period (default: 100)
      thinning: 1              # Keep every nth sample (default: 1)
      randomize_order: true    # Randomize variable update order (default: true)

      # Simulated Annealing parameters:
      beta_range: [1.0, 1.0]                    # Temperature schedule [beta_min, beta_max]
      proposal_acceptance_criteria: "Gibbs"     # "Gibbs" or "Metropolis"

      # Metropolis sampler parameters:
      # temperature: 1.0         # Temperature T for acceptance probability (default: 1.0)
      # num_sweeps: 1000         # Number of MCMC sweeps (default: 1000)
      # burn_in: 100             # Burn-in period (default: 100)
      # thinning: 1              # Keep every nth sample (default: 1)

      # Parallel Tempering parameters:
      # num_replicas: 8          # Number of temperature replicas (default: 8)
      # T_min: 1.0               # Minimum temperature (default: 1.0)
      # T_max: 4.0               # Maximum temperature (default: 4.0)
      # swap_interval: 10        # Sweeps between swap attempts (default: 10)
      # num_sweeps: 1000         # Total number of sweeps (default: 1000)
      # burn_in: 100             # Burn-in period (default: 100)
      # thinning: 1              # Keep every nth sample (default: 1)

      # Exact sampler parameters:
      # max_variables: 20        # Maximum number of variables (N <= 20, default: 20)

      # Gumbel-max sampler parameters:
      # max_variables: 20        # Maximum number of variables (N <= 20, default: 20)

      # Tabu sampler parameters:
      # tenure: null             # Tabu list size (default: auto)
      # timeout: 20              # Timeout in seconds (default: 20)

      # D-Wave quantum/hybrid parameters:
      # solver: null             # Optional: specify solver name
      # time_limit: 5            # For hybrid samplers (seconds)

# Learned Model Configuration
learned_model:
  # Should match true model topology
  n_visible: 10
  n_hidden: 0
  model_type: "fvbm"  # "fvbm" (Fully Visible BM) or "rbm" (Restricted BM)
  connectivity: "dense"  # "dense" or "sparse"
  connectivity_density: 0.7  # Only used if connectivity is "sparse" (0.0-1.0)

  # Initial parameter scale (smaller than true model)
  init_linear_scale: 0.1
  init_quadratic_scale: 0.1

# Training Configuration
training:
  batch_size: 5000
  n_epochs: 100
  learning_rate: 0.01
  optimizer: "adam"  # "sgd" or "adam"

  # ===== TRAINING MODE =====
  # Choose training strategy: "cd" (Contrastive Divergence) or "pcd" (Persistent CD)
  mode: "pcd"  # "cd" or "pcd"

  # CD-k Configuration (when mode: "cd")
  cd_k: 1  # Number of Gibbs steps for CD-k (default: 1)

  # PCD Configuration (when mode: "pcd")
  pcd:
    num_chains: 100         # Number of persistent chains (default: 100)
    k_steps: 10             # MCMC steps per parameter update (default: 10)
    initialize_from: "random"  # "random" or "data" (default: "random")

  # Model sampling during training
  model_sample_size: 100
  prefactor: 1.0

  # ===== SAMPLER CONFIGURATION =====
  sampler:
    type: "gibbs"
    # ===== PRODUCTION SAMPLERS (Recommended) =====
    # MCMC Samplers (work with both CD and PCD):
    #   "gibbs"                  - ⭐ BEST for BM training, PCD-compatible
    #   "gibbs_gpu"              - ⭐ GPU version for large-scale (N>1000)
    #   "metropolis"             - Alternative MCMC sampler
    #   "metropolis_gpu"         - GPU version
    #   "parallel_tempering"     - For complex landscapes
    #   "parallel_tempering_gpu" - GPU version
    #
    # GPU Recommendations:
    #   Small (N ≤ 100):      gibbs + CD-1
    #   Medium (N ~ 1000):    gibbs_gpu + CD-10
    #   Large (N ≥ 10000):    gibbs_gpu + PCD (k=10, chains=256)
    #   Very Large (N~10^5):  gibbs_gpu + PCD (k=5, chains=1000)
    #
    # ===== EXACT SAMPLERS (Testing Only, N ≤ 20) =====
    #   "exact"                  - Brute force enumeration
    #   "gumbel_max"             - Independent exact samples
    #
    # ===== OPTIMIZERS (Not for training, use for optimization tasks) =====
    #   "simulated_annealing"    - ⚠️ Optimization tool, biased sampling
    #   "simulated_annealing_gpu" - GPU version
    #   "population_annealing_gpu" - Population-based optimizer
    #   "tabu"                   - Tabu search optimizer
    #   "steepest_descent"       - Local search optimizer
    #   "greedy"                 - Greedy optimizer
    #
    # ===== BASELINE =====
    #   "random"                 - Uniform random baseline
    params:
      # Gibbs sampler parameters:
      num_sweeps: 1000         # Number of MCMC sweeps (default: 1000)
      burn_in: 100             # Burn-in period (default: 100, set to 0 for PCD)
      thinning: 1              # Keep every nth sample (default: 1)
      randomize_order: true    # Randomize variable update order (default: true)

      # Gibbs GPU parameters (additional):
      # num_chains: 32           # Number of parallel chains (default: 32)
      # use_cuda: true           # Use CUDA if available (default: true)

      # Metropolis parameters:
      # temperature: 1.0         # Temperature T (default: 1.0)
      # num_sweeps: 1000
      # burn_in: 100
      # thinning: 1

      # Metropolis GPU parameters (additional):
      # num_chains: 32
      # use_cuda: true

      # Parallel Tempering parameters:
      # num_replicas: 8          # Number of temperature replicas (default: 8)
      # T_min: 1.0               # Minimum temperature (default: 1.0)
      # T_max: 4.0               # Maximum temperature (default: 4.0)
      # swap_interval: 10        # Sweeps between swap attempts (default: 10)
      # num_sweeps: 1000
      # burn_in: 100
      # thinning: 1

      # Parallel Tempering GPU parameters (additional):
      # use_cuda: true

  # Optimizer parameters
  optimizer_params:
    betas: [0.85, 0.999]  # Slightly lower momentum for noisy BM gradients
    eps: 1.0e-7
    weight_decay: 0.0     # Use dedicated regularization instead

  # Gradient clipping (prevents divergence)
  gradient_clipping:
    enabled: true
    method: "norm"        # "norm" or "value"
    max_norm: 1.0         # Maximum gradient norm
    max_value: 0.5        # Maximum gradient value (if method="value")

  # Regularization (prevents overfitting and unbounded parameters)
  regularization:
    linear_l2: 0.001      # L2 penalty on linear biases
    quadratic_l2: 0.01    # L2 penalty on quadratic weights
    quadratic_l1: 0.00    # L1 penalty for sparsity (optional)

  # Learning rate scheduling
  lr_scheduler:
    enabled: true
    type: "plateau"       # "plateau", "step", "cosine", or "exponential"
    # Plateau-specific params
    factor: 0.5           # Reduce LR by this factor
    patience: 15          # Epochs without improvement before reducing
    min_lr: 1.0e-5        # Minimum learning rate
    monitor: "val_loss"   # Metric to monitor
    # Step-specific params
    step_size: 100        # For type="step"
    gamma: 0.5            # For type="step" or "exponential"
    # Cosine-specific params
    T_max: 50             # For type="cosine"
    eta_min: 1.0e-5       # For type="cosine"

  # Early stopping
  early_stopping:
    enabled: false              # Enable/disable early stopping
    patience: 20                # Number of epochs without improvement before stopping
    min_delta: 0.0001           # Minimum change to qualify as improvement
    metric: "val_loss"          # Metric to monitor: "val_loss", "train_loss", "grad_norm"
    mode: "min"                 # "min" for loss, "max" for accuracy-like metrics
    restore_best_weights: true  # Restore best model weights after early stopping

  # Hidden unit handling (only if n_hidden > 0)
  hidden_kind: null  # "exact-disc", "sampling", or null (null for fully visible BM)

  # Checkpointing
  save_best_model: true
  checkpoint_dir: "outputs/checkpoints"

# Logging and Visualization
logging:
  log_interval: 1  # Log every N epochs
  save_plots: true
  plot_dir: "outputs/plots"

  # Metrics to track
  track_metrics:
    - "loss"
    - "grad_norm"
    - "beta"
    - "val_loss"

# Benchmark Configuration
# Benchmarks samplers by computing KL divergence from true Boltzmann distribution
benchmark:
  # Samplers to benchmark
  # See data generation section above for full list of available samplers
  # Recommended: "gibbs", "simulated_annealing", "steepest_descent", "exact", "random"
  samplers:
    - "gibbs"                 # Gibbs MCMC (theoretically correct, best quality)
    - "simulated_annealing"   # SA baseline (good for optimization)
    - "steepest_descent"      # Fast local search
    - "exact"                 # Brute force (slow for N > 10)
    - "random"                # Uniform random baseline

  # Range of problem sizes (number of variables)
  n_variables_range: [2, 3, 4, 5, 6, 7, 8, 9, 10]

  # Number of samples per benchmark run
  n_samples: 10000

  # Model configuration for benchmarking (always dense FVBM)
  model_type: "fvbm"
  connectivity: "dense"
  linear_bias_scale: 1.0
  quadratic_weight_scale: 1.5

# Paths (relative to bm_pipeline/ directory when running main.py directly,
#        or relative to project root when using run scripts)
paths:
  data_dir: "outputs/data"
  model_dir: "outputs/models"
  log_dir: "outputs/logs"
