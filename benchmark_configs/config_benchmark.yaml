# Boltzmann Machine Sampler Benchmark Configuration
#
# This configuration is specifically for benchmarking different sampling methods
# by computing KL divergence between empirical distributions from samplers
# and the exact Boltzmann distribution.
#
# Usage:
#   python main.py --mode benchmark --config benchmark_configs/config_benchmark.yaml

# Random seed for reproducibility
seed: 42

# Device configuration
device:
  use_cuda: "auto"  # "auto", "cuda", or "cpu"

# Benchmark Configuration
benchmark:
  # === Samplers to Benchmark === List of samplers to test. Each sampler will be evaluated across all problem sizes.
  # Available samplers:
  #   Classical (Free, Local):
  #     "gibbs"                - Gibbs MCMC sampler (theoretically correct)
  #     "simulated_annealing"  - MCMC-based annealing (good for optimization)
  #     "steepest_descent"     - Local search (fast, finds local optima)
  #     "exact"                - Brute force enumeration (only feasible for N ≤ 20)
  #     "random"               - Uniform random baseline
  #
  #   Quantum (Requires D-Wave Leap Account):
  #     "dwave", "advantage"   - D-Wave quantum annealer (QPU)
  #
  #   Hybrid (Requires D-Wave Leap Account):
  #     "hybrid", "hybrid_bqm", "hybrid_dqm", "kerberos"
  samplers:
    - "gibbs"                 # Gibbs MCMC (best quality)
    - "simulated_annealing"   # SA baseline
    - "steepest_descent"      # Fast local search
    - "exact"                 # Brute force (perfect but slow)
    - "random"                # Random baseline

  # === Problem Sizes ===
  # Range of problem sizes (number of variables) to test.
  # Computational complexity grows as O(2^N) for exact distribution computation.
  # Recommended: Keep N ≤ 20 for exact computations, N ≤ 10 for practical benchmarking.
  n_variables_range: [2, 3, 4, 5, 6, 7, 8, 9, 10]

  # === Sampling Configuration ===
  # Number of samples to generate per benchmark run.
  # Higher values give better empirical distribution estimates but take longer.
  n_samples: 1000

  # === True Model Parameters ===
  # Configuration for the random Boltzmann Machines used in benchmarking.
  # Each benchmark run creates a new random FVBM with these settings.
  model_type: "fvbm"            # Always use fully visible BM for benchmarking
  connectivity: "dense"         # Always use dense connectivity
  linear_bias_scale: 1.0        # Scale for random linear biases
  quadratic_weight_scale: 1.5   # Scale for random quadratic weights

  # === Sampler-Specific Parameters ===
  # Default parameters used for all samplers during benchmarking.
  # These can be overridden per-sampler if needed.
  sampler_params:
    # Common parameters:
    num_reads: 1000  # Number of samples to request (will be capped to n_samples)

    # Gibbs sampler parameters:
    num_sweeps: 1000         # Number of MCMC sweeps
    burn_in: 100             # Burn-in period
    thinning: 1              # Keep every nth sample
    randomize_order: true    # Randomize variable update order

    # Simulated Annealing parameters:
    beta_range: [1.0, 1.0]                    # Temperature schedule [beta_min, beta_max]
    proposal_acceptance_criteria: "Gibbs"     # "Gibbs" or "Metropolis"

    # Tabu sampler parameters:
    tenure: null             # Tabu list size (default: auto)
    timeout: 20              # Timeout in seconds

    # D-Wave quantum/hybrid parameters:
    solver: null             # Optional: specify solver name
    time_limit: 5            # For hybrid samplers (seconds)

# Data/Model Generation (for compatibility with run_manager)
# This section is only used for prefactor value during sampling.
# The actual models are created dynamically by the benchmark suite.
data_generation:
  prefactor: 1.0  # Energy scaling factor

# Logging and Visualization
logging:
  log_interval: 1         # Log interval (not used in benchmarking)
  save_plots: true        # Save visualization plots
  plot_dir: "outputs/plots"

# Output Paths
paths:
  data_dir: "outputs/data"      # Where benchmark_results.csv and benchmark_summary.csv are saved
  model_dir: "outputs/models"   # Not used in benchmarking
  log_dir: "outputs/logs"       # Not used in benchmarking
