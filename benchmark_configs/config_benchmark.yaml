# Boltzmann Machine Sampler Benchmark Configuration
#
# This configuration is specifically for benchmarking different sampling methods
# by computing KL divergence between empirical distributions from samplers
# and the exact Boltzmann distribution.
#
# Usage:
#   python main.py --mode benchmark --config benchmark_configs/config_benchmark.yaml

# Random seed for reproducibility
seed: 42

# Device configuration
device:
  use_cuda: "auto"  # "auto", "cuda", or "cpu"

# Benchmark Configuration
benchmark:
  # === Samplers to Benchmark === List of samplers to test. Each sampler will be evaluated across all problem sizes.
  # Available samplers:
  #   Classical MCMC (Free, Local):
  #     "gibbs"                - Gibbs MCMC sampler (best for probability sampling)
  #     "metropolis"           - Metropolis-Hastings MCMC (temperature-controlled)
  #     "parallel_tempering"   - Parallel Tempering (replica exchange, best for complex landscapes)
  #     "simulated_annealing"  - MCMC-based annealing (good general purpose)
  #
  #   Exact/Quasi-Exact (Free, Local, small N only):
  #     "exact"                - Brute force exact sampling (only for N ≤ 20 variables)
  #     "gumbel_max"           - Gumbel-max exact sampling (independent samples, N ≤ 20)
  #
  #   Baseline:
  #     "random"               - Uniform random (baseline)
  #
  #   Quantum (Requires D-Wave Leap Account):
  #     "dwave"                - D-Wave quantum annealer (QPU)
  #     "advantage"            - D-Wave Advantage system (alias for dwave)
  samplers:
    - "exact"
    - "gumbel_max"

  # === Problem Sizes ===
  # Range of problem sizes (number of variables) to test.
  # Computational complexity grows as O(2^N) for exact distribution computation.
  # Recommended: Keep N ≤ 20 for exact computations, N ≤ 10 for practical benchmarking.
  n_variables_range: [2, 3, 4, 5, 6, 7, 8, 9, 10]

  # === Sampling Configuration ===
  # Number of samples to generate per benchmark run.
  # Higher values give better empirical distribution estimates but take longer.
  n_samples: 1000

  # === True Model Parameters ===
  # Configuration for the random Boltzmann Machines used in benchmarking.
  # Each benchmark run creates a new random FVBM with these settings.
  model_type: "fvbm"            # Always use fully visible BM for benchmarking
  connectivity: "dense"         # Always use dense connectivity
  linear_bias_scale: 1.0        # Scale for random linear biases
  quadratic_weight_scale: 1.5   # Scale for random quadratic weights

  # === Metrics Configuration ===
  # Configure which metrics to compute during benchmarking
  metrics:
    # List of enabled metrics
    enabled:
      - "kl_divergence"           # Forward KL: D_KL(p_true || p_empirical)
      - "reverse_kl_divergence"   # Reverse KL: D_KL(p_empirical || p_true)
      - "total_variation"         # Total Variation Distance
      - "mmd"                     # Maximum Mean Discrepancy
      - "moment_matching"         # First and second moment matching error

    # MMD kernel configuration
    mmd_kernel: "rbf_hamming"     # "rbf_hamming" or "linear_hamming"
    mmd_kernel_bandwidth: 1.0     # Bandwidth for RBF kernel

  # === Sampler-Specific Parameters ===
  # Default parameters used for all samplers during benchmarking.
  # These can be overridden per-sampler if needed.
  sampler_params:
    # Common parameters:
    num_reads: 1000  # Number of samples to request (will be capped to n_samples)

    # Gibbs sampler parameters:
    num_sweeps: 1000         # Number of MCMC sweeps
    burn_in: 100             # Burn-in period
    thinning: 1              # Keep every nth sample
    randomize_order: true    # Randomize variable update order

    # Simulated Annealing parameters:
    beta_range: [1.0, 1.0]                    # Temperature schedule [beta_min, beta_max]
    proposal_acceptance_criteria: "Gibbs"     # "Gibbs" or "Metropolis"

    # Metropolis sampler parameters:
    # temperature: 1.0         # Temperature T for acceptance probability (default: 1.0)
    # num_sweeps: 1000         # Number of MCMC sweeps (default: 1000)
    # burn_in: 100             # Burn-in period (default: 100)
    # thinning: 1              # Keep every nth sample (default: 1)

    # Parallel Tempering parameters:
    # num_replicas: 8          # Number of temperature replicas (default: 8)
    # T_min: 1.0               # Minimum temperature (default: 1.0)
    # T_max: 4.0               # Maximum temperature (default: 4.0)
    # swap_interval: 10        # Sweeps between swap attempts (default: 10)
    # num_sweeps: 1000         # Total number of sweeps (default: 1000)
    # burn_in: 100             # Burn-in period (default: 100)
    # thinning: 1              # Keep every nth sample (default: 1)

    # Exact sampler parameters:
    # max_variables: 20        # Maximum number of variables (N <= 20, default: 20)

    # Gumbel-max sampler parameters:
    # max_variables: 20        # Maximum number of variables (N <= 20, default: 20)

    # Tabu sampler parameters:
    tenure: null             # Tabu list size (default: auto)
    timeout: 20              # Timeout in seconds

    # D-Wave quantum/hybrid parameters:
    solver: null             # Optional: specify solver name
    time_limit: 5            # For hybrid samplers (seconds)

# Data/Model Generation (for compatibility with run_manager)
# This section is only used for prefactor value during sampling.
# The actual models are created dynamically by the benchmark suite.
data_generation:
  prefactor: 1.0  # Energy scaling factor

# Logging and Visualization
logging:
  log_interval: 1         # Log interval (not used in benchmarking)
  save_plots: true        # Save visualization plots
  plot_dir: "outputs/plots"

# Output Paths
paths:
  data_dir: "outputs/data"      # Where benchmark_results.csv and benchmark_summary.csv are saved
  model_dir: "outputs/models"   # Not used in benchmarking
  log_dir: "outputs/logs"       # Not used in benchmarking
